{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "714df8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e632c09",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Our data will is in the DATASET folder split into our TRAIN and TEST datasets. Each folder contains one folder for each of our labels. Our next step will be to preprocess our data. \n",
    "\n",
    "* We use ImageDataGenerator to do some transformations of the images\n",
    "* Then we can apply the transformations to the directories for our train and test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20762ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/TRAIN'\n",
    "test_dir = 'data/TEST'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3347d3",
   "metadata": {},
   "source": [
    "## Data Augmentation and Preprocessing\n",
    "\n",
    "- The `ImageDataGenerator` is a way to create \"new\" images by transforming our data.\n",
    "- Because we will use a pretrained model as a base (VGG19), we will need to preprocess our images to match the format expected by the model, most pretrained models have implemented a `preprocess_input` function for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0281467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "train_datagen = ImageDataGenerator(width_shift_range = 0.1,\n",
    "                                  horizontal_flip = True,\n",
    "                                  rescale = 1./255,\n",
    "                                  validation_split = 0.2,\n",
    "                                  preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(rescale =1./255,\n",
    "                                 validation_split = 0.2,\n",
    "                                 preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc285a4",
   "metadata": {},
   "source": [
    "## Loading and transforming our data from the different folders\n",
    "\n",
    "We will give the directory to the image data into a generator object, we will specify a few parameters, \n",
    "- directory: path to the images\n",
    "- target_size: The size of the images\n",
    "- color_mode: 'rgb' since they are in color\n",
    "- class_mode: 'categorial'\n",
    "- batch_size: how many image we will load at a time\n",
    "- subset: training or test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2a371cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 866 images belonging to 5 classes.\n",
      "Found 92 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                   target_size = (224,224),\n",
    "                                                   color_mode = 'rgb',\n",
    "                                                   class_mode = 'categorical',\n",
    "                                                    batch_size = 16,\n",
    "                                                   subset = 'training')\n",
    "validation_generator = test_datagen.flow_from_directory(directory = test_dir,\n",
    "                                                       target_size = (224,224),\n",
    "                                                       color_mode = 'rgb',\n",
    "                                                       class_mode = 'categorical',\n",
    "                                                       batch_size = 16,\n",
    "                                                       subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c058182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'downdog': 0, 'goddess': 1, 'plank': 2, 'tree': 3, 'warrior2': 4}\n",
      "{'downdog': 0, 'goddess': 1, 'plank': 2, 'tree': 3, 'warrior2': 4}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.class_indices)\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "122c213c",
   "metadata": {},
   "source": [
    "We will use the VGG19 model you can read more about the requirements and considerations for this model in the documentation (https://keras.io/api/applications/vgg/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75ae9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20024384 (76.39 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 20024384 (76.39 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "model = VGG19(include_top = False,weights = 'imagenet',input_shape= (224,224,3))\n",
    "\n",
    "# Freeze the imported layers so they cannot be retrained.\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b247471",
   "metadata": {},
   "source": [
    "### Adding flattening and dense layers\n",
    "\n",
    "Right now, our model is missing a top to actually classify our features. Let's add them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "433463e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 125445    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20149829 (76.87 MB)\n",
      "Trainable params: 125445 (490.02 KB)\n",
      "Non-trainable params: 20024384 (76.39 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(5,activation = 'softmax'))\n",
    "\n",
    "# Summarize.\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17de85fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 300s 5s/step - loss: 0.7823 - accuracy: 0.7691 - val_loss: 0.3406 - val_accuracy: 0.9348\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "# Compile and fit the model. Use the Adam optimizer and crossentropical loss. \n",
    "# Use the validation data argument during fitting to include your validation data.\n",
    "optimizer = Adam(learning_rate = 0.0001)\n",
    "new_model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "history = new_model.fit(train_generator,\n",
    "                        epochs=1, \n",
    "                        batch_size=64,\n",
    "                        validation_data=validation_generator\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ca76f",
   "metadata": {},
   "source": [
    "# Predicting the class of your image\n",
    "\n",
    "Let's take this bad boy for a spin! Can your image get properly identified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f32a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(1, 224, 224, 3)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "(1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'downdog'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Predict the class of your picture.\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(\"./test_folder/downward_dog_new.jpg\", target_size = (224, 224))\n",
    "\n",
    "\n",
    "img_nparray = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "print(img_nparray.shape)\n",
    "#convert image to array\n",
    "\n",
    "x = preprocess_input(img_nparray).reshape((1,224,224,3))\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "prediction = new_model.predict(x)\n",
    "\n",
    "print(prediction.shape)\n",
    "\n",
    "# create a list containing the class labels\n",
    "class_labels = ['downdog', 'goddess', 'plank', 'tree', 'warrior2']\n",
    "\n",
    "# find the index of the class with maximum score\n",
    "pred = np.argmax(prediction, axis=-1)\n",
    "class_labels[pred[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
